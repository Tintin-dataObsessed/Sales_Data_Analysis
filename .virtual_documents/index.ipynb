





#Importing relevant libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline


#Overview of the data.
df = pd.read_csv('Case_Study_Data.csv', index_col=0)
df.head(10)


#The data has a date index like a timeseries so let me understand the columns and data types by using info.
df.info()


'''
Things to note include;
Most columns are object which means they could have data types of both string and null values except Qunatity.
They are 333405 entries in the data set.
Thye are 6 columns including an index.
Confirming they are in timeseries or sequential it says from August 18, 2024, 9:32 PM to October 10, 2024, 6:19 PM
They does not seem to be a main ID column or transaction ID column.
'''





#I convert the Unit Price column to interger.
df['UNIT PRICE'] = pd.to_numeric(df['UNIT PRICE'], errors='coerce')
# convert the other columns to string values.
df['ANONYMIZED CATEGORY'] = df['ANONYMIZED CATEGORY'].astype(str)
df['ANONYMIZED PRODUCT'] = df['ANONYMIZED PRODUCT'].astype(str)
df['ANONYMIZED BUSINESS'] = df['ANONYMIZED BUSINESS'].astype(str)
df['ANONYMIZED LOCATION'] = df['ANONYMIZED LOCATION'].astype(str)


df.info()





'''
I start my inital cleaning with Inspecting the dataset for missing values..
For Missing values (NaN) I took the true and false values in isna and summed them to show the total number of NaN values.
'''
df.isna().sum()


#I have decided to fill the rows with a mean value.
mean_unit_price = df['UNIT PRICE'].mean()
df['UNIT PRICE'].fillna(mean_unit_price, inplace=True)
#I have to check that they are dropped properly.
df.isna().sum()





#This shows the number of duplicated rows in the dataset.
df[df.duplicated].value_counts()


'''
My assumption is this is sales data and so people have bought the same product more than once, 
There are over 55001 entries with duplicate combinations, we can perform a groupby to understand how many times each combination appears.
but I must separate the date because I need it for the time series.
'''
df.reset_index(inplace=True)
df


#I can now check for combinations of products sold that are duplicates. 
duplicates = df.groupby(['DATE','ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION','QUANTITY']).size()
print(duplicates)


'''
I was conflicted because this is sales data, and dropping these values especially because they will impact sales volumne was risky.
It is possible to have the same item bought in the same day, or even over a different day but same combination.
'''
df_aggregated = df.groupby(['DATE', 'ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION','QUANTITY']).agg({'UNIT PRICE': 'mean'}).reset_index()
print(df_aggregated.head())


#This helps me to if it worked. 
duplicates = df_aggregated.groupby(['DATE','ANONYMIZED CATEGORY', 'ANONYMIZED PRODUCT', 'ANONYMIZED BUSINESS', 'ANONYMIZED LOCATION','QUANTITY']).size()
print(duplicates)


'''
This shows they are 329797 unique combinations in the new dataset all with 1 count, which is good and shows the duplicates are not there.
'''








#Converting the DATE column into a specific date dtype.
df_aggregated['DATE'] = pd.to_datetime(df_aggregated['DATE'], format='%B %d, %Y, %I:%M %p')
#Created the new column with Month and Year using dt.strftime('%B-%Y') which turns dates into strings in a format.
df_aggregated['Month-Year'] = df_aggregated['DATE'].dt.strftime('%B-%Y') 


#I can now check to see if the new Month-Year has been added from the Date column.
df_aggregated.head(5)


#This is a timeseries so the date column must be made an index again.
df_aggregated.set_index('DATE', inplace=True)


df_aggregated





df_aggregated.columns


# Group by 'ANONYMIZED CATEGORY' and 'ANONYMIZED BUSINESS'
grouped_data = df_aggregated.groupby(['ANONYMIZED CATEGORY']).agg(TOTAL_QUANTITY=('QUANTITY', 'sum'),UNIT_PRICE=('UNIT PRICE', 'mean')).reset_index()
grouped_data['TOTAL_VALUE'] = grouped_data['TOTAL_QUANTITY'] * grouped_data['UNIT_PRICE']

grouped_data_Category = grouped_data[grouped_data['TOTAL_QUANTITY'] >= 10000]
grouped_data_Category = grouped_data_Category.sort_values(by='TOTAL_QUANTITY', ascending=False)
print(grouped_data_Category.head())



# Plot the bar chart
plt.figure(figsize=(12, 6))
plt.bar(grouped_data_Category['ANONYMIZED CATEGORY'], grouped_data_Category['TOTAL_QUANTITY'], color='green')

# Customize the chart
plt.xlabel("ANONYMIZED CATEGORY")
plt.ylabel("Total Quantity Sold")
plt.title("Total Quantity by Category")
plt.xticks(rotation=45)  # Rotate labels for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.show()


grouped_data = grouped_data.sort_values(by='TOTAL_VALUE', ascending=False)
print(grouped_data.head())



# Plot the bar chart
plt.figure(figsize=(12, 6))
plt.bar(grouped_data_Category['ANONYMIZED CATEGORY'], grouped_data_Category['TOTAL_VALUE'], color='lightgreen')

# Customize the chart
plt.xlabel("ANONYMIZED CATEGORY")
plt.ylabel("Total Value sold")
plt.title("Total Value by Category")
plt.xticks(rotation=45)  # Rotate labels for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)


# Show the plot
plt.show()


# Group by 'ANONYMIZED CATEGORY' and 'ANONYMIZED BUSINESS'
grouped_data_business = df_aggregated.groupby(['ANONYMIZED BUSINESS']).agg(TOTAL_QUANTITY=('QUANTITY', 'sum'),UNIT_PRICE=('UNIT PRICE', 'mean')).reset_index()
grouped_data_business['TOTAL_VALUE'] = grouped_data_business['TOTAL_QUANTITY'] * grouped_data_business['UNIT_PRICE']
grouped_data_business = grouped_data_business[grouped_data_business['TOTAL_QUANTITY'] >= 2000]
grouped_data_business = grouped_data_business.sort_values(by='TOTAL_QUANTITY', ascending=False)
print(grouped_data_business.head())





# Plot the bar chart
plt.figure(figsize=(12, 6))
plt.bar(grouped_data_business['ANONYMIZED BUSINESS'], grouped_data_business['TOTAL_QUANTITY'], color='green')
# Customize the chart
plt.xlabel("ANONYMIZED BUSINESS")
plt.ylabel("Total Quantity Sold")
plt.title("Total Quantity by BUSINESS")
plt.xticks(rotation=45)  # Rotate labels for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.show()



grouped_data_business = grouped_data_business.sort_values(by='TOTAL_VALUE', ascending=False)
print(grouped_data_business.head())



# Plot the bar chart
plt.figure(figsize=(12, 6))
plt.bar(grouped_data_business['ANONYMIZED BUSINESS'], grouped_data_business['TOTAL_VALUE'], color='lightgreen')

# Customize the chart
plt.xlabel("ANONYMIZED BUSINESS")
plt.ylabel("Total Value sold")
plt.title("Total Value by Category")
plt.xticks(rotation=45)  # Rotate labels for readability
plt.grid(axis='y', linestyle='--', alpha=0.7)


# Show the plot
plt.show()



df_aggregated['TOTAL_VALUE'] = df_aggregated['QUANTITY'] * df_aggregated['UNIT PRICE']
sales_trends = df_aggregated.groupby('Month-Year').agg(TOTAL_QUANTITY=('QUANTITY', 'sum'),TOTAL_VALUE=('TOTAL_VALUE', 'sum')).reset_index()
df_aggregated


plt.figure(figsize=(14, 7))

# Plot Total Quantity
plt.subplot(2, 1, 1)
plt.plot(sales_trends['Month-Year'].astype(str), sales_trends['TOTAL_QUANTITY'], marker='o', color='#477CA8', label='Total Quantity')
plt.title('Sales Trends: Total Quantity by Month-Year')
plt.xlabel('Month-Year')
plt.ylabel('Total Quantity')
plt.xticks(rotation=45)
plt.legend()

# Plot Total Value
plt.subplot(2, 1, 2)
plt.plot(sales_trends['Month-Year'].astype(str), sales_trends['TOTAL_VALUE'], marker='o', color='#CB3335', label='Total Value')
plt.title('Sales Trends: Total Value by Month-Year')
plt.xlabel('Month-Year')
plt.ylabel('Total Value')
plt.xticks(rotation=45)
plt.legend()

plt.tight_layout()
plt.show()


# Calculate total quantity and total value by product
product_performance = df_aggregated.groupby('ANONYMIZED PRODUCT').agg(
    TOTAL_QUANTITY=('QUANTITY', 'sum'),
    TOTAL_VALUE=('TOTAL_VALUE', 'sum')
).reset_index()


# Identify the top 5 most frequently purchased products based on Quantity
top_5_quantity = product_performance.nlargest(5, 'TOTAL_QUANTITY')

# Identify the top 5 most valuable products based on Value
top_5_value = product_performance.nlargest(5, 'TOTAL_VALUE')

# Print the results
print("Top 5 Most Frequently Purchased Products (by Quantity):")
print(top_5_quantity)

print("\nTop 5 Most Valuable Products (by Value):")
print(top_5_value)


# Set up the figure and axes for the plots
fig, axs = plt.subplots(2, 1, figsize=(10, 10))

# Bar chart for Top 5 Products by Quantity
axs[0].bar(top_5_quantity['ANONYMIZED PRODUCT'], top_5_quantity['TOTAL_QUANTITY'], color='orange')
axs[0].set_title('Top 5 Most Frequently Purchased Products (by Quantity)')
axs[0].set_xlabel('Product')
axs[0].set_ylabel('Total Quantity')
axs[0].grid(axis='y', linestyle='--', alpha=0.7)

# Bar chart for Top 5 Products by Value
axs[1].bar(top_5_value['ANONYMIZED PRODUCT'], top_5_value['TOTAL_VALUE'], color='green')
axs[1].set_title('Top 5 Most Valuable Products (by Value)')
axs[1].set_xlabel('Product')
axs[1].set_ylabel('Total Value')
axs[1].grid(axis='y', linestyle='--', alpha=0.7)

# Adjust layout
plt.tight_layout()
plt.show()








grouped_data_business_v2 = df_aggregated.groupby(['ANONYMIZED BUSINESS']).agg(TOTAL_QUANTITY=('QUANTITY', 'sum'),UNIT_PRICE=('UNIT PRICE', 'mean')).reset_index()
grouped_data_business_v2['TOTAL_VALUE'] = grouped_data_business_v2['TOTAL_QUANTITY'] * grouped_data_business_v2['UNIT_PRICE']
print(grouped_data_business.head())



def segment_quantity(row):
    if row['TOTAL_QUANTITY'] < 100:
        return 'Low Quantity'
    elif 500 <= row['TOTAL_QUANTITY'] < 10000:
        return 'Medium Quantity'
    else:
        return 'High Quantity'

# Apply the segmentation function
grouped_data_business_v2['Quantity_Segment'] = grouped_data_business_v2.apply(segment_quantity, axis=1)
grouped_data_business_v2


plt.figure(figsize=(6,6))
grouped_data_business_v2['Quantity_Segment'].value_counts().plot.pie(autopct='%1.1f%%', colors=['#477CA8', '#6BB24C', '#FFA500'] )

plt.title('Proportion of Business Segments')
plt.ylabel('')
plt.show()







def segment_value(row):
    if row['TOTAL_VALUE'] < 10000:
        return 'Low Value'
    elif 10000 <= row['TOTAL_VALUE'] < 50000:  # Fixed missing range
        return 'Medium Value'
    else:
        return 'High Value'


# Apply the segmentation function
grouped_data_business_v2['Value_Segment'] = grouped_data_business_v2.apply(segment_value, axis=1)
grouped_data_business_v2


plt.figure(figsize=(8,5))
sns.countplot(data=grouped_data_business_v2, x='Value_Segment', palette='Set1')

plt.xlabel('Value Segment')
plt.ylabel('Number of Businesses')
plt.title('Number of Businesses')
plt.show()



frequency = grouped_data_business_v2['ANONYMIZED BUSINESS'].value_counts().reset_index()
frequency.columns = ['ANONYMIZED BUSINESS', 'Transaction_Frequency']
frequency



def segment_Frequency(row):
    if row['Transaction_Frequency'] <= 1:
        return 'Low'
    elif 3 <= row['Transaction_Frequency'] <= 5:
        return 'Medium'
    else:
        return 'High'

# Apply the segmentation function
frequency['Segment_Frequency'] = frequency.apply(segment_Frequency, axis=1)
print(frequency)


# Sample data
transaction_frequency = [1, 2, 3,]  # Extend this list as needed
total_transactions = [4800, 0, 0]   # Corresponding total transactions

# Create bar chart
plt.bar(transaction_frequency, total_transactions, color='green')
plt.xlabel('Transaction Frequency')
plt.ylabel('Total Transactions')
plt.title('Total Transactions by Frequency')
plt.xticks(transaction_frequency)
plt.show()



frequency





df_aggregated.columns


from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Ensure 'DATE' is a datetime index
df_aggregated.index = pd.to_datetime(df_aggregated.index)

# Resample to monthly total sales
df_monthly = df_aggregated.resample('D').sum()

model = ExponentialSmoothing(
    df_monthly['TOTAL_VALUE'],  
    trend='add', 
    seasonal='add', 
    seasonal_periods=100  # Adjust this based on your data
).fit()


# Forecast for the next 3 months
forecast = model.forecast(90)

# Plot results
plt.figure(figsize=(12, 6))
plt.plot(df_monthly['TOTAL_VALUE'], label='Actual Sales', color='#477CA8')
plt.plot(forecast, label='Forecast', linestyle='dashed', color='#CB3335')
plt.xlabel('Date')
plt.ylabel('Total Sales')
plt.title('3-Month Sales Forecast (Holt-Winters)')
plt.legend()
plt.show()





# Aggregate data (e.g., daily sales)
daily_sales = df_aggregated.groupby('DATE').agg({'QUANTITY': 'sum', 'TOTAL_VALUE': 'sum'}).reset_index()


#help you spot anomalies Easily
plt.figure(figsize=(14, 7))
plt.plot(daily_sales['DATE'], daily_sales['TOTAL_VALUE'], label='Daily Sales Value', color='blue')
plt.plot(daily_sales['DATE'], daily_sales['QUANTITY'], label='Daily Quantity Sold', color='orange')
plt.title('Daily Sales Performance')
plt.xlabel('Date')
plt.ylabel('Sales')
plt.legend()
plt.show()


mean_value = daily_sales['TOTAL_VALUE'].mean()
std_value = daily_sales['TOTAL_VALUE'].std()

# Define a threshold for anomalies
threshold = 3  # For example, 3 standard deviations

# Identify anomalies
daily_sales['Anomaly'] = (daily_sales['TOTAL_VALUE'] > mean_value + threshold * std_value) | (daily_sales['TOTAL_VALUE'] < mean_value - threshold * std_value)
anomalies = daily_sales[daily_sales['Anomaly']]


daily_sales['Moving_Average'] = daily_sales['TOTAL_VALUE'].rolling(window=7).mean()
daily_sales['Anomaly'] = (daily_sales['TOTAL_VALUE'] > daily_sales['Moving_Average'] * 1.5) | (daily_sales['TOTAL_VALUE'] < daily_sales['Moving_Average'] * 0.5)
anomalies = daily_sales[daily_sales['Anomaly']]


anomalies








# Scatter plot with regression line
plt.figure(figsize=(10, 6))
sns.regplot(x='QUANTITY', y='TOTAL_VALUE', data=daily_sales, marker='o', color='blue')
plt.title('Relationship Between Quantity and Value')
plt.xlabel('Quantity Sold')
plt.ylabel('Sales Value')
plt.grid()
plt.show()


correlation = daily_sales['QUANTITY'].corr(daily_sales['TOTAL_VALUE'])
print(f'Correlation between Quantity and Value: {correlation:.2f}')



